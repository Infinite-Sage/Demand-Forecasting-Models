{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "014f90a8",
   "metadata": {},
   "source": [
    "\n",
    "# Home Decor Demand Forecasting with Explainable AI\n",
    "\n",
    "This notebook uses Explainable AI (XAI) to analyze and interpret machine learning models predicting customer demand in the **Home Decor** category. It explains the top purchase drivers using SHAP (Shapley values), enabling clear communication with both technical and non-technical stakeholders.\n",
    "\n",
    "## Home Decor Sales Forecasting: Explainable AI (XAI) Model Interpretation\n",
    "\n",
    "- Purpose: Help marketing, supply chain, and exec teams understand what drives next month's home decor sales using explainable machine learning.\n",
    "\n",
    "- This script uses SHAP (SHapley Additive exPlanations) to explain how two models — Random Forest and K-Nearest Neighbors (KNN) — make predictions.\n",
    "\n",
    "## Project Goals\n",
    "\n",
    "- Identify features that drive future home decor purchases\n",
    "- Compare feature explanations from Random Forest vs K-Nearest Neighbors (KNN)\n",
    "- Provide insights for Marketing, Inventory, and Strategy teams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cbed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# 1. Load Libraries\n",
    "# -------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import shap\n",
    "import sys\n",
    "\n",
    "# Ensure correct Python version\n",
    "assert (\n",
    "    sys.version_info.major == 3 and sys.version_info.minor == 10\n",
    "), \"Please ensure you're using Python 3.10 for compatibility\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb17ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# 2. Load Data & Models\n",
    "# -------------------------\n",
    "\n",
    "# We're using a subset of customer transaction data for faster SHAP explanation.\n",
    "# X_train/X_test: customer behavior data (e.g., past purchases)\n",
    "# y_train/y_test: actual future home_decor sales (used only for training and validation)\n",
    "X_train = pd.read_csv(\"data/X_train.csv\").sample(500, random_state=42)\n",
    "X_test = pd.read_csv(\"data/X_test.csv\").sample(500, random_state=42)\n",
    "y_train = pd.read_csv(\"data/y_train.csv\")[\"nextmonth__home_decor\"].sample(500, random_state=42)\n",
    "y_test = pd.read_csv(\"data/y_test.csv\")[\"nextmonth__home_decor\"].sample(500, random_state=42)\n",
    "\n",
    "# Load our two machine learning models: a Random Forest and a K-Nearest Neighbors model\n",
    "model = joblib.load(\"data/model.pkl\")         # Random Forest Regressor\n",
    "knn_model = joblib.load(\"data/knn_model.pkl\") # K-Nearest Neighbors Regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b28484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 2A. Model Training Reference \n",
    "# -------------------------\n",
    "# This block shows how the models were trained. No need to run this unless you’re retraining.\n",
    "\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# model = RandomForestRegressor(...)\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# knn_model = KNeighborsRegressor(...)\n",
    "# knn_model.fit(X_train, y_train)\n",
    "\n",
    "# -------------------------\n",
    "# 2B. Explainability Technique\n",
    "# -------------------------\n",
    "# We’re using SHAP, a tool from game theory that tells us which features (like past purchases)\n",
    "# had the biggest influence on the model’s predictions. This helps stakeholders *understand* why the model made a prediction.\n",
    "xai = \"shap\"\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88fbcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# 3. SHAP for Random Forest\n",
    "# -------------------------\n",
    "# Use SHAP’s TreeExplainer since Random Forest is a tree-based model.\n",
    "# This calculates how much each feature (e.g. past sales, category purchases) affected predicted demand.\n",
    "explainer_rf = shap.TreeExplainer(model)\n",
    "shap_values_rf = explainer_rf.shap_values(X_test)\n",
    "\n",
    "# Average the importance scores to rank the top features\n",
    "importances_rf = np.abs(shap_values_rf).mean(axis=0)\n",
    "feat_imp_rf = pd.DataFrame({\n",
    "    \"feature\": X_test.columns,\n",
    "    \"importance\": importances_rf\n",
    "}).sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "# Show the five most important features\n",
    "top_feats = feat_imp_rf.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6c34df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# 4. SHAP for KNN Model\n",
    "# -------------------------\n",
    "\n",
    "# KNN isn’t tree-based, so we use SHAP’s KernelExplainer (model-agnostic).\n",
    "# It approximates which features influenced the model for a small sample.\n",
    "X_test_sample = X_test.sample(50, random_state=42)\n",
    "background = shap.kmeans(X_test, 5) # A smart summary sample to speed things up\n",
    "\n",
    "knn_explainer = shap.KernelExplainer(knn_model.predict, background, seed=42)\n",
    "X_test_sample = X_test_sample.iloc[:10] # Use 10 rows to keep processing fast\n",
    "knn_shap_values = knn_explainer.shap_values(X_test_sample)\n",
    "\n",
    "# Rank top 5 features influencing the KNN model\n",
    "importances_knn = np.abs(knn_shap_values).mean(axis=0)\n",
    "feat_imp_knn = pd.DataFrame({\n",
    "    \"feature\": X_test_sample.columns,\n",
    "    \"importance\": importances_knn\n",
    "}).sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "top_feats_knn = feat_imp_knn.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec1745c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# 5. Consistency Between Models\n",
    "# -------------------------\n",
    "# Let's compare the top 5 features from both models and see if they agree.\n",
    "# We use cosine similarity — a measure of how similar the two importance rankings are.\n",
    "# A score near 1.0 = strong agreement.\n",
    "\n",
    "merged = pd.merge(top_feats, top_feats_knn, on=\"feature\", suffixes=(\"_rf\", \"_knn\"))\n",
    "\n",
    "consistency = round(\n",
    "    cosine_similarity(\n",
    "        [merged[\"importance_rf\"].values],\n",
    "        [merged[\"importance_knn\"].values]\n",
    "    )[0][0], 2\n",
    ")\n",
    "\n",
    "reliable = \"yes\" if consistency >= 0.80 else \"no\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bc98d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# 6. Final Output (for Execs)\n",
    "# -------------------------\n",
    "# This block prints the results — helpful for presentations or reports.\n",
    "print(\"\\nXAI Method Used:\", xai.upper())\n",
    "print(\"Top 5 Features from Random Forest:\\n\", top_feats)\n",
    "print(\"Top 5 Features from KNN:\\n\", top_feats_knn)\n",
    "print(\"Feature Importance Consistency Score:\", consistency)\n",
    "print(\"Are Interpretations Reliable?\", reliable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7086b82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 7. Business Interpretation\n",
    "# -------------------------\n",
    "# Key takeaways for marketing, inventory, and product teams:\n",
    "#\n",
    "# - 'lag1', 'lag2': If a customer bought home decor 1–2 months ago, they’re likely to buy again.\n",
    "# - 'sma_4m': A rolling average of past 4-month purchases — great for tracking patterns.\n",
    "# - 'colourful_essentials': Buying related categories (e.g., colorful decor) predicts future purchases.\n",
    "#\n",
    "# MARKETING: Target customers with these past behaviors using personalized offers.\n",
    "# INVENTORY: Prepare stock based on expected demand from these signals.\n",
    "# STRATEGY: Create bundle offers that include top co-purchased categories.\n",
    "\n",
    "# -------------------------\n",
    "# 8. SHAP Summary Plot (Optional Visual)\n",
    "# -------------------------\n",
    "# This visualization shows which features most influenced predictions across all test samples.\n",
    "# Positive values = feature pushed the prediction up (more demand)\n",
    "# Negative values = pushed it down (less demand)\n",
    "shap.summary_plot(shap_values_rf, X_test, show=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e959dc55",
   "metadata": {},
   "source": [
    "### Interpretation of the SHAP Summary Plot (for Non-Tech + Business Teams):\n",
    "This SHAP plot answers:\n",
    "“What features most influence the model’s prediction of next month’s home décor purchase volume?”\n",
    "\n",
    "Key Visual Takeaways:\n",
    "Top Drivers of Demand\n",
    "\n",
    "The top 5 features consistently driving predictions are:\n",
    "\n",
    " - lag2 and lag1: Home décor purchases made 1–2 months ago\n",
    "\n",
    " - sma_4m and sma_2m: Short-term spending trends\n",
    "\n",
    " - sma_2m__colourful_essentials: Recent purchases in a related category (cross-sell signal)\n",
    "\n",
    "These customers are repeat buyers or recently active — prime marketing targets.\n",
    "\n",
    "Color Tells a Story\n",
    "\n",
    "- Red = high feature value (e.g. high past spending)\n",
    "\n",
    "- Blue = low feature value (e.g. no past purchases)\n",
    "\n",
    "You can clearly see that high recent activity (red) in these top features pushes predictions up, meaning more demand.\n",
    "\n",
    "Strong Positive Push\n",
    "- Features like lag2, sma_4m, and sma_2m__colourful_essentials consistently push predictions higher when values are high (shown as red points far to the right).\n",
    "\n",
    "These are reliable buying signals.\n",
    "\n",
    "Secondary Drivers\n",
    "Other contributing categories:\n",
    "\n",
    "- soft_furnishings, home_storage, children_s_accessories, quirky_stationery\n",
    "\n",
    "Suggests opportunities for category bundles or personalized recommendations.\n",
    "\n",
    "Negative or Neutral Influence\n",
    "- Features like quarter and logsales (log-transformed overall sales) cluster around 0, showing minimal predictive value.\n",
    "\n",
    " These may be dropped in future model optimization.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
